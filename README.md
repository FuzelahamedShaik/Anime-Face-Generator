![image](https://github.com/user-attachments/assets/05acdb53-1ed0-4849-9441-335ca7d95f92)

The training loss of the discriminator stays low whereas generator loss is quite stable. The losses of the generator and discriminator are too far from each other to provide optimal training and performance of the network. The samples created improved eventually, but I couldnâ€™t notice any improvements in the latter epochs. A deeper architecture would have benefited from a greater number of epochs as it could optimize more parameters. For further modeling, one can try using different optimizers to generator and discriminator. To improve the performance of this model one can try to increase the dropout ratios in the discriminator architecture. This can solve the vanishing gradient problem. More hyperparameter techniques such as smaller learning rates, add noise to the discriminator layers. 
